{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/xiyichen/smplify-x-partial/blob/master/notebooks/Keypoints_Blending_with_Confidence_Calibration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAxa-09S8V4U"
   },
   "source": [
    "# This notebook allows you to perform keypoints blending and visualization for [OpenPose BODY_25 format model](https://cmu-perceptual-computing-lab.github.io/openpose/web/html/doc/md_doc_02_output.html#pose-output-format-body_25) and [MMPose Halpe format model](https://mmpose.readthedocs.io/en/latest/topics/wholebody.html#halpe-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sHhW5MVUVV_"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Efhr6S45UT9_",
    "outputId": "3e618be6-99fa-424e-a37a-e65bd073a919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace\n",
    "import os\n",
    "from os.path import exists, join, basename, splitext\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import json\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uILcwa7kpnW2"
   },
   "source": [
    "# Install OpenPose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wcbef2rmUDz"
   },
   "source": [
    "# Install MMPose and download Halpe format model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "9a4fca8053be4e1d8b729ff6118a9c45",
      "49ff66d45ce9430bab637e510a915f98",
      "d35864af678f4aab8367c98275669298",
      "bc7c3bbb8d594e9bb7a0bc3f672288ba",
      "f56f80425ffe4e7dbc6e155ee85142d6",
      "6d32e9eac96944a08c3ec0ec1124c015",
      "a7b5662f6ec547038a2ca3a4c7b67dab",
      "390a2ba23c9f4abfb7566d87368ca96f",
      "798be96747fa41b784565aef4d247296",
      "de9527c232d047219c8fae4383afb5a5",
      "70f026f132e143eca82ecdbab127a126",
      "819211b258314224918b09ee27f625ff",
      "68e61eeefd034219a483e9f0347f26c2",
      "7d954a770a664caeae10d48a9324c49e",
      "f6ad6df56b0f4afbb0d5e7f0ea686a60",
      "bd52b8cd96fa4dda817aaa45e08bed5e",
      "75a11e28a5fb49cb818d76d4cc3b4767",
      "7fe716927ba044c0afa588f87a45e95a",
      "009a800baa884ef9b857d651db49e512",
      "4e124465e1ca42059cda022fa78618e9",
      "911d276ce0e1456d907aa91e83b366c8",
      "a69c23a58aa6471996f1845ec47b3dca"
     ]
    },
    "id": "dmODBTDjmY5v",
    "outputId": "ac8cf8ff-6302-4ab8-f4ee-f8762528039c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/mmpose\n",
      "load checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_halpe_384x288_dark_plus-d13c2588_20211021.pth\n",
      "load checkpoint from http path: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/mmpose\n",
    "import cv2\n",
    "from mmpose.apis import (inference_top_down_pose_model, init_pose_model,\n",
    "                         vis_pose_result, process_mmdet_results)\n",
    "from mmdet.apis import inference_detector, init_detector\n",
    "local_runtime = False\n",
    "\n",
    "try:\n",
    "  from google.colab.patches import cv2_imshow  # for image visualization in colab\n",
    "except:\n",
    "  local_runtime = True\n",
    "\n",
    "pose_config = 'configs/wholebody/2d_kpt_sview_rgb_img/topdown_heatmap/halpe/hrnet_w48_halpe_384x288_dark_plus.py'\n",
    "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_halpe_384x288_dark_plus-d13c2588_20211021.pth'\n",
    "det_config = 'demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py'\n",
    "det_checkpoint = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n",
    "\n",
    "# initialize pose model\n",
    "pose_model = init_pose_model(pose_config, pose_checkpoint)\n",
    "# initialize detector\n",
    "det_model = init_detector(det_config, det_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PopS_NenDdwT"
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nKZjY9w5Sv4G"
   },
   "outputs": [],
   "source": [
    "import mmcv\n",
    "import math\n",
    "def imshow_keypoints_modified(img,\n",
    "                     pose_result,\n",
    "                     skeleton=None,\n",
    "                     kpts_score_thr=[0.3] * 136,\n",
    "                     pose_kpt_color=None,\n",
    "                     pose_link_color=None,\n",
    "                     radius=4,\n",
    "                     thickness=1,\n",
    "                     show_keypoint_weight=False):\n",
    "    img = mmcv.imread(img)\n",
    "    img_h, img_w, _ = img.shape\n",
    "\n",
    "    for kpts in pose_result:\n",
    "\n",
    "        kpts = np.array(kpts, copy=False)\n",
    "\n",
    "        # draw each point on image\n",
    "        if pose_kpt_color is not None:\n",
    "            assert len(pose_kpt_color) == len(kpts)\n",
    "\n",
    "            for kid, kpt in enumerate(kpts):\n",
    "                x_coord, y_coord, kpt_score = int(kpt[0]), int(kpt[1]), kpt[2]\n",
    "                if kpt_score < kpts_score_thr[kid] or pose_kpt_color[kid] is None:\n",
    "                    # skip the point that should not be drawn\n",
    "                    continue\n",
    "\n",
    "                color = tuple(int(c) for c in pose_kpt_color[kid])\n",
    "                if show_keypoint_weight:\n",
    "                    img_copy = img.copy()\n",
    "                    cv2.circle(img_copy, (int(x_coord), int(y_coord)), radius,\n",
    "                               color, -1)\n",
    "                    transparency = max(0, min(1, kpt_score))\n",
    "                    cv2.addWeighted(\n",
    "                        img_copy,\n",
    "                        transparency,\n",
    "                        img,\n",
    "                        1 - transparency,\n",
    "                        0,\n",
    "                        dst=img)\n",
    "                else:\n",
    "                    cv2.circle(img, (int(x_coord), int(y_coord)), radius,\n",
    "                               color, -1)\n",
    "\n",
    "        # draw links\n",
    "        if skeleton is not None and pose_link_color is not None:\n",
    "            assert len(pose_link_color) == len(skeleton)\n",
    "\n",
    "            for sk_id, sk in enumerate(skeleton):\n",
    "                pos1 = (int(kpts[sk[0], 0]), int(kpts[sk[0], 1]))\n",
    "                pos2 = (int(kpts[sk[1], 0]), int(kpts[sk[1], 1]))\n",
    "\n",
    "                if (pos1[0] <= 0 or pos1[0] >= img_w or pos1[1] <= 0\n",
    "                        or pos1[1] >= img_h or pos2[0] <= 0 or pos2[0] >= img_w\n",
    "                        or pos2[1] <= 0 or pos2[1] >= img_h\n",
    "                        or kpts[sk[0], 2] < kpts_score_thr[sk[0]]\n",
    "                        or kpts[sk[1], 2] < kpts_score_thr[sk[1]]\n",
    "                        or pose_link_color[sk_id] is None):\n",
    "                    # skip the link that should not be drawn\n",
    "                    continue\n",
    "                color = tuple(int(c) for c in pose_link_color[sk_id])\n",
    "                if show_keypoint_weight:\n",
    "                    img_copy = img.copy()\n",
    "                    X = (pos1[0], pos2[0])\n",
    "                    Y = (pos1[1], pos2[1])\n",
    "                    mX = np.mean(X)\n",
    "                    mY = np.mean(Y)\n",
    "                    length = ((Y[0] - Y[1])**2 + (X[0] - X[1])**2)**0.5\n",
    "                    angle = math.degrees(math.atan2(Y[0] - Y[1], X[0] - X[1]))\n",
    "                    stickwidth = 2\n",
    "                    polygon = cv2.ellipse2Poly(\n",
    "                        (int(mX), int(mY)), (int(length / 2), int(stickwidth)),\n",
    "                        int(angle), 0, 360, 1)\n",
    "                    cv2.fillConvexPoly(img_copy, polygon, color)\n",
    "                    transparency = max(\n",
    "                        0, min(1, 0.5 * (kpts[sk[0], 2] + kpts[sk[1], 2])))\n",
    "                    cv2.addWeighted(\n",
    "                        img_copy,\n",
    "                        transparency,\n",
    "                        img,\n",
    "                        1 - transparency,\n",
    "                        0,\n",
    "                        dst=img)\n",
    "                else:\n",
    "                    cv2.line(img, pos1, pos2, color, thickness=thickness)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dMzIDojXSmuz"
   },
   "outputs": [],
   "source": [
    "from mmcv.visualization.image import imshow\n",
    "from mmcv.image import imwrite\n",
    "from mmpose.core import imshow_bboxes\n",
    "def show_result_modified(img,\n",
    "                    result,\n",
    "                    skeleton=None,\n",
    "                    kpt_score_thr=0.3,\n",
    "                    bbox_color='green',\n",
    "                    pose_kpt_color=None,\n",
    "                    pose_link_color=None,\n",
    "                    text_color='white',\n",
    "                    radius=4,\n",
    "                    thickness=1,\n",
    "                    font_scale=0.5,\n",
    "                    bbox_thickness=1,\n",
    "                    win_name='',\n",
    "                    show=False,\n",
    "                    show_keypoint_weight=False,\n",
    "                    wait_time=0,\n",
    "                    out_file=None):\n",
    "\n",
    "        img = mmcv.imread(img)\n",
    "        img = img.copy()\n",
    "\n",
    "        bbox_result = []\n",
    "        bbox_labels = []\n",
    "        pose_result = []\n",
    "        for res in result:\n",
    "            if 'bbox' in res:\n",
    "                bbox_result.append(res['bbox'])\n",
    "                bbox_labels.append(res.get('label', None))\n",
    "            pose_result.append(res['keypoints'])\n",
    "\n",
    "        if bbox_result:\n",
    "            bboxes = np.vstack(bbox_result)\n",
    "            # draw bounding boxes\n",
    "            imshow_bboxes(\n",
    "                img,\n",
    "                bboxes,\n",
    "                labels=bbox_labels,\n",
    "                colors=bbox_color,\n",
    "                text_color=text_color,\n",
    "                thickness=bbox_thickness,\n",
    "                font_scale=font_scale,\n",
    "                show=False)\n",
    "\n",
    "        if pose_result:\n",
    "            imshow_keypoints_modified(img, pose_result, skeleton, kpt_score_thr,\n",
    "                             pose_kpt_color, pose_link_color, radius,\n",
    "                             thickness)\n",
    "\n",
    "        if show:\n",
    "            imshow(img, win_name, wait_time)\n",
    "\n",
    "        if out_file is not None:\n",
    "            imwrite(img, out_file)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "AADixMBDY9jH"
   },
   "outputs": [],
   "source": [
    "def vis_pose_result_body_25(model,\n",
    "                    img,\n",
    "                    result,\n",
    "                    radius=4,\n",
    "                    thickness=1,\n",
    "                    kpt_score_thr=0.3,\n",
    "                    bbox_color='green',\n",
    "                    dataset='TopDownCocoDataset',\n",
    "                    dataset_info=None,\n",
    "                    show=False,\n",
    "                    out_file=None):\n",
    "    \"\"\"Visualize the detection results on the image.\n",
    "    Args:\n",
    "        model (nn.Module): The loaded detector.\n",
    "        img (str | np.ndarray): Image filename or loaded image.\n",
    "        result (list[dict]): The results to draw over `img`\n",
    "                (bbox_result, pose_result).\n",
    "        radius (int): Radius of circles.\n",
    "        thickness (int): Thickness of lines.\n",
    "        kpt_score_thr (float): The threshold to visualize the keypoints.\n",
    "        skeleton (list[tuple()]): Default None.\n",
    "        show (bool):  Whether to show the image. Default True.\n",
    "        out_file (str|None): The filename of the output visualization image.\n",
    "    \"\"\"\n",
    "\n",
    "    # get dataset info\n",
    "    palette_body = np.array([[255, 0, 85], [255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], [255, 0, 0],\n",
    "        [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [255, 0, 170], [170, 0, 255], [255, 0, 255], [85, 0, 255],\n",
    "        [0, 0, 255], [0, 0, 255], [0, 0, 255], [0, 255, 255], [0, 255, 255], [0, 255, 255]])\n",
    "    body_skeleton = np.array([[1,8], [1,2], [1,5], [2,3], [3,4], [5,6], [6,7], [8,9], [9,10], [10,11], \n",
    "    [8,12], [12,13], [13,14], [1,0], [0,15], [15,17], [0,16], [16,18], [14,19], [19,20], [14,21], [11,22], [22,23], [11,24]])\n",
    "    body_link_color = palette_body[body_skeleton[:,1]]\n",
    "\n",
    "    palette_hand = np.array([[100, 100, 100], [100, 0, 0], [150, 0, 0], [200, 0, 0], [255, 0, 0], [100, 100, 0], [150, 150, 0], [200, 200, 0], [255, 255, 0],\n",
    "        [0, 100, 50], [0, 150, 75], [0, 200, 100], [0, 255, 125], [0, 50, 100], [0, 75, 150], [0, 100, 200], [0, 125, 255], [100, 0, 100], [150, 0, 150], [200, 0, 200], [255, 0, 255]])\n",
    "    hand_skeleton = np.array([[0,1], [1,2], [2,3], [3,4], [0,5], [5,6], [6,7], [7,8], [0,9], [9,10], [10,11], [11,12], [0,13], [13,14], [14,15], [15,16], [0,17], [17,18], [18,19], [19,20]])\n",
    "    hand_skeleton_left = hand_skeleton + 25\n",
    "    hand_skeleton_right = hand_skeleton_left + 21\n",
    "    hand_link_color = palette_hand[hand_skeleton[:,1]]\n",
    "    pose_link_color = np.concatenate((body_link_color, hand_link_color, hand_link_color), axis=0)\n",
    "    skeleton = np.concatenate((body_skeleton, hand_skeleton_left, hand_skeleton_right), axis=0)\n",
    "    palette_face = np.array([[255, 255, 255]] * 68)\n",
    "    pose_kpt_color = np.concatenate((palette_body, palette_hand, palette_hand, palette_face), axis=0)\n",
    "    \n",
    "    img = show_result_modified(\n",
    "        img,\n",
    "        result,\n",
    "        skeleton,\n",
    "        radius=radius,\n",
    "        thickness=thickness,\n",
    "        pose_kpt_color=pose_kpt_color,\n",
    "        pose_link_color=pose_link_color,\n",
    "        kpt_score_thr=kpt_score_thr,\n",
    "        bbox_color=bbox_color,\n",
    "        show=show,\n",
    "        out_file=out_file)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6K1aPuY_Ut1c"
   },
   "outputs": [],
   "source": [
    "pose_model.show_result = show_result_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "403kUwAt2APg"
   },
   "outputs": [],
   "source": [
    "Keypoints = namedtuple('Keypoints',\n",
    "                       ['keypoints', 'gender_gt', 'gender_pd'])\n",
    "\n",
    "Keypoints.__new__.__defaults__ = (None,) * len(Keypoints._fields)\n",
    "\n",
    "def read_keypoints(keypoint_fn, use_hands=True, use_face=True,\n",
    "                   use_face_contour=False, orders=['body', 'hands', 'face']):\n",
    "    with open(keypoint_fn) as keypoint_file:\n",
    "        data = json.load(keypoint_file)\n",
    "\n",
    "    keypoints = []\n",
    "\n",
    "    gender_pd = []\n",
    "    gender_gt = []\n",
    "    for idx, person_data in enumerate(data['people']):\n",
    "        body_keypoints = np.array(person_data['pose_keypoints_2d'],\n",
    "                                  dtype=np.float32)\n",
    "        body_keypoints = body_keypoints.reshape([-1, 3])\n",
    "        if use_hands:\n",
    "            left_hand_keyp = np.array(\n",
    "                person_data['hand_left_keypoints_2d'],\n",
    "                dtype=np.float32).reshape([-1, 3])\n",
    "            right_hand_keyp = np.array(\n",
    "                person_data['hand_right_keypoints_2d'],\n",
    "                dtype=np.float32).reshape([-1, 3])\n",
    "            hands_keypoints = np.concatenate([left_hand_keyp, right_hand_keyp], axis=0)\n",
    "        if use_face:\n",
    "            face_keypoints = np.array(\n",
    "                person_data['face_keypoints_2d'],\n",
    "                dtype=np.float32).reshape([-1, 3])[:68, :]\n",
    "\n",
    "        if orders[0] == 'body':\n",
    "          curr_keypoints = body_keypoints\n",
    "        elif orders[0] == 'hands':\n",
    "          curr_keypoints = hands_keypoints\n",
    "        else:\n",
    "          curr_keypoints = face_keypoints\n",
    "        \n",
    "        if orders[1] == 'body':\n",
    "          curr_keypoints = np.concatenate([curr_keypoints, body_keypoints])\n",
    "        elif orders[1] == 'hands':\n",
    "          curr_keypoints = np.concatenate([curr_keypoints, hands_keypoints])\n",
    "        else:\n",
    "          curr_keypoints = np.concatenate([curr_keypoints, face_keypoints])\n",
    "\n",
    "        if orders[2] == 'body':\n",
    "          curr_keypoints = np.concatenate([curr_keypoints, body_keypoints])\n",
    "        elif orders[2] == 'hands':\n",
    "          curr_keypoints = np.concatenate([curr_keypoints, hands_keypoints])\n",
    "        else:\n",
    "          curr_keypoints = np.concatenate([curr_keypoints, face_keypoints])\n",
    "\n",
    "        keypoints.append(curr_keypoints)\n",
    "\n",
    "    return keypoints[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5Q_MaV9-IuV"
   },
   "source": [
    "# Upload images and set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "id": "UWsEMF20_D-J"
   },
   "outputs": [],
   "source": [
    "IMAGES_ROOT = '//mimer/NOBACKUP/groups/snic2022-22-770/Frontiers_framework/Case26/'\n",
    "files = glob.glob(os.path.join(IMAGES_ROOT, '**/*.jpg')) + \\\n",
    "     glob.glob(os.path.join(IMAGES_ROOT, '**/*.jpeg')) + \\\n",
    "     glob.glob(os.path.join(IMAGES_ROOT, '**/*.png'))\n",
    "if files == []:\n",
    "  print('No images found')\n",
    "else:\n",
    "  IMAGES_PATH = '/' + os.path.join(*(files[0].split('/')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mimer/NOBACKUP/groups/snic2022-22-770/Frontiers_framework/Case26/images'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGES_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N5RCzhkmSGf"
   },
   "source": [
    "# Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uFv0JKPjbbN",
    "outputId": "5001b397-ca3f-4f09-b2d3-83bc4b5c397e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cephyr/users/yuanq/Alvis/content\n",
      "rm: cannot remove 'openpose_results': No such file or directory\n",
      "rm: cannot remove 'mmpose_results': No such file or directory\n",
      "rm: cannot remove 'blending_results': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mkdir /cephyr/users/yuanq/Alvis/content\n",
    "%cd /cephyr/users/yuanq/Alvis/content\n",
    "!rm -r openpose_results\n",
    "!rm -r mmpose_results\n",
    "!rm -r blending_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fa6JKyz0oU4v",
    "outputId": "70d7b744-218a-4d95-a4aa-c5ee221a4a8e"
   },
   "outputs": [],
   "source": [
    "# Run OpenPose\n",
    "OPENPOSE_RES_DIR = '/cephyr/users/yuanq/Alvis/content/openpose_results'\n",
    "!mkdir $OPENPOSE_RES_DIR\n",
    "\n",
    "#!cd /content/openpose && ./build/examples/openpose/openpose.bin --image_dir $IMAGES_PATH --write_json $OPENPOSE_RES_DIR --face --hand --display 0 --render_pose 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "lk5Ivm12A0NL"
   },
   "outputs": [],
   "source": [
    "# Run MMPose\n",
    "#MMPOSE_RES_DIR = '~/content/mmpose_results'\n",
    "MMPOSE_RES_DIR = '/cephyr/users/yuanq/Alvis/content/mmpose_results'  # Replace with your actual home directory path\n",
    "\n",
    "#!mkdir $MMPOSE_RES_DIR\n",
    "\n",
    "for fn in glob.glob(os.path.join(IMAGES_PATH, '*')):\n",
    "  img_name = fn.split('/')[-1].split('.')[0]\n",
    "  img = os.path.join(IMAGES_PATH, img_name + '.jpg')\n",
    "\n",
    "  # inference detection\n",
    "  mmdet_results = inference_detector(det_model, img)\n",
    "\n",
    "  # extract person (COCO_ID=1) bounding boxes from the detection results\n",
    "  person_results = process_mmdet_results(mmdet_results, cat_id=1)\n",
    "\n",
    "  # inference pose\n",
    "  pose_results, returned_outputs = inference_top_down_pose_model(pose_model,\n",
    "                                                                img,\n",
    "                                                                person_results,\n",
    "                                                                format='xyxy',\n",
    "                                                                dataset=pose_model.cfg.data.test.type)\n",
    "  param_dict = {}\n",
    "  param_dict['people'] = [{\"person_id\":[-1]}]\n",
    "  keypoints_flatten = pose_results[0]['keypoints'].flatten().tolist()\n",
    "  param_dict['people'][0]['pose_keypoints_2d'] = keypoints_flatten[:26*3]\n",
    "  param_dict['people'][0]['face_keypoints_2d'] = keypoints_flatten[26*3:94*3]\n",
    "  param_dict['people'][0]['hand_left_keypoints_2d'] = keypoints_flatten[94*3:115*3]\n",
    "  param_dict['people'][0]['hand_right_keypoints_2d'] = keypoints_flatten[115*3:]\n",
    "  with open(os.path.join(MMPOSE_RES_DIR, img_name + '_mmpose.json'), 'w') as outfile:\n",
    "    json.dump(param_dict, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cephyr/users/yuanq/Alvis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "home_directory = os.path.expanduser('~')\n",
    "print(home_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='/cephyr/users/yuanq/Alvis/content/mmpose_results/MyVideo_1_140_mmpose.json' mode='w' encoding='UTF-8'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENPOSE_RES_DIR = '/cephyr/users/yuanq/Alvis/content/openpose_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "wyVauRDUDN0z"
   },
   "outputs": [],
   "source": [
    "# Load keypoints\n",
    "openpose_files = {}\n",
    "mmpose_files = {}\n",
    "for fn in glob.glob(os.path.join(IMAGES_PATH, '*')):\n",
    "  img_name = fn.split('/')[-1].split('.')[0]\n",
    "  openpose_fn = os.path.join(OPENPOSE_RES_DIR, img_name + '_keypoints.json')\n",
    "  openpose_files[img_name] = read_keypoints(openpose_fn, use_hands=True, use_face=True, use_face_contour=True)\n",
    "  mmpose_fn = os.path.join(MMPOSE_RES_DIR, img_name + '_mmpose.json')\n",
    "  mmpose_files[img_name] = read_keypoints(mmpose_fn, use_hands=True, use_face=True, use_face_contour=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "6HputQMmE9uM"
   },
   "outputs": [],
   "source": [
    "# Matching keypoints for different formats\n",
    "pairs = {\"Nose\": {'MMPose': 0, 'OpenPose': 0},\n",
    "      \"LEye\": {'MMPose': 1, 'OpenPose': 16},\n",
    "      \"REye\": {'MMPose': 2, 'OpenPose': 15},\n",
    "      \"LEar\": {'MMPose': 3, 'OpenPose': 18},\n",
    "      \"REar\": {'MMPose': 4, 'OpenPose': 17},\n",
    "      \"LShoulder\": {'MMPose': 5, 'OpenPose': 5},\n",
    "      \"RShoulder\": {'MMPose': 6, 'OpenPose': 2},\n",
    "      \"LElbow\": {'MMPose': 7, 'OpenPose': 6},\n",
    "      \"RElbow\": {'MMPose': 8, 'OpenPose': 3},\n",
    "      \"LWrist\": {'MMPose': 9, 'OpenPose': 7},\n",
    "      \"RWrist\": {'MMPose': 10, 'OpenPose': 4},\n",
    "      \"LHip\": {'MMPose': 11, 'OpenPose': 12},\n",
    "      \"RHip\": {'MMPose': 12, 'OpenPose': 9},\n",
    "      \"LKnee\": {'MMPose': 13, 'OpenPose': 13},\n",
    "      \"RKnee\": {'MMPose': 14, 'OpenPose': 10},\n",
    "      \"LAnkle\": {'MMPose': 15, 'OpenPose': 14},\n",
    "      \"RAnkle\": {'MMPose': 16, 'OpenPose': 11},\n",
    "      # \"Head\": {'MMPose': 17}, omitted since no counterpart in OpenPose\n",
    "      \"Neck\": {'MMPose': 18, 'OpenPose': 1},\n",
    "      \"Hip\": {'MMPose': 19, 'OpenPose': 8},\n",
    "      \"LBigToe\": {'MMPose': 20, 'OpenPose': 19},\n",
    "      \"RBigToe\": {'MMPose': 21, 'OpenPose': 22},\n",
    "      \"LSmallToe\": {'MMPose': 22, 'OpenPose': 20},\n",
    "      \"RSmallToe\":{'MMPose': 23, 'OpenPose': 23},\n",
    "      \"LHeel\": {'MMPose': 24, 'OpenPose': 21},\n",
    "      \"RHeel\": {'MMPose': 25, 'OpenPose': 24}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "1dbWgRbtFaLK"
   },
   "outputs": [],
   "source": [
    "openpose_pose_len = 25\n",
    "mmpose_pose_len = 26\n",
    "for i in range(21):\n",
    "  key = 'left_hand_' + str(i+1)\n",
    "  pairs[key] = {}\n",
    "  pairs[key]['OpenPose'] = openpose_pose_len + i\n",
    "  pairs[key]['MMPose'] = mmpose_pose_len + i\n",
    "for i in range(21):\n",
    "  key = 'right_hand_' + str(i+1)\n",
    "  pairs[key] = {}\n",
    "  pairs[key]['OpenPose'] = openpose_pose_len + 21 + i\n",
    "  pairs[key]['MMPose'] = mmpose_pose_len + 21 + i\n",
    "for i in range(68):\n",
    "  key = 'face_' + str(i+1)\n",
    "  pairs[key] = {}\n",
    "  pairs[key]['OpenPose'] = openpose_pose_len + 42 + i\n",
    "  pairs[key]['MMPose'] = mmpose_pose_len + 42 + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2wBdk5z-mRWn",
    "outputId": "a80eab66-3b81-4182-94b1-355a7460bd7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-27 22:38:25--  https://polybox.ethz.ch/index.php/s/UHTisMSR5RzMi0X/download\n",
      "Resolving polybox.ethz.ch (polybox.ethz.ch)... 129.132.71.243\n",
      "Connecting to polybox.ethz.ch (polybox.ethz.ch)|129.132.71.243|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7526 (7.3K) [application/zip]\n",
      "Saving to: '/cephyr/users/yuanq/Alvis/content/shhq_heuristics.zip'\n",
      "\n",
      "/cephyr/users/yuanq 100%[===================>]   7.35K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-27 22:38:25 (1.72 GB/s) - '/cephyr/users/yuanq/Alvis/content/shhq_heuristics.zip' saved [7526/7526]\n",
      "\n",
      "Archive:  /cephyr/users/yuanq/Alvis/content/shhq_heuristics.zip\n",
      "  inflating: /cephyr/users/yuanq/Alvis/content/heuristics/mmpose_means.json  \n",
      "  inflating: /cephyr/users/yuanq/Alvis/content/heuristics/mmpose_stds.json  \n",
      "  inflating: /cephyr/users/yuanq/Alvis/content/heuristics/openpose_means.json  \n",
      "  inflating: /cephyr/users/yuanq/Alvis/content/heuristics/openpose_stds.json  \n"
     ]
    }
   ],
   "source": [
    "# Get the heuristics for per-keypoint statistics, used for confidence score calibration\n",
    "\n",
    "!mkdir /cephyr/users/yuanq/Alvis/content/heuristics\n",
    "!wget -O /cephyr/users/yuanq/Alvis/content/shhq_heuristics.zip https://polybox.ethz.ch/index.php/s/UHTisMSR5RzMi0X/download\n",
    "!unzip -n /cephyr/users/yuanq/Alvis/content/shhq_heuristics.zip -d /cephyr/users/yuanq/Alvis/content/heuristics\n",
    "with open('/cephyr/users/yuanq/Alvis/content/heuristics/openpose_means.json', 'r') as f:\n",
    "  openpose_means = json.load(f)\n",
    "with open('/cephyr/users/yuanq/Alvis/content/heuristics/openpose_stds.json', 'r') as f:\n",
    "  openpose_stds = json.load(f)\n",
    "with open('/cephyr/users/yuanq/Alvis/content/heuristics/mmpose_means.json', 'r') as f:\n",
    "  mmpose_means = json.load(f)\n",
    "with open('/cephyr/users/yuanq/Alvis/content/heuristics/mmpose_stds.json', 'r') as f:\n",
    "  mmpose_stds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "30cixK_lGqAQ"
   },
   "outputs": [],
   "source": [
    "# Blending\n",
    "BLENDING_RES_DIR = '/cephyr/users/yuanq/Alvis/content/blending_results'\n",
    "!mkdir $BLENDING_RES_DIR\n",
    "for fn in glob.glob(os.path.join(IMAGES_PATH, '*')):\n",
    "  img_name = fn.split('/')[-1].split('.')[0]\n",
    "  blended_current = np.zeros((135, 3))\n",
    "  for key in pairs:\n",
    "    if 'face' in key:\n",
    "      openpose_conf = openpose_files[img_name][pairs[key]['OpenPose']][2]\n",
    "      openpose_conf = np.clip(openpose_conf, 0, 1)\n",
    "      blended_current[pairs[key]['OpenPose']][0] = openpose_files[img_name][pairs[key]['OpenPose']][0]\n",
    "      blended_current[pairs[key]['OpenPose']][1] = openpose_files[img_name][pairs[key]['OpenPose']][1]\n",
    "      blended_current[pairs[key]['OpenPose']][2] = openpose_conf\n",
    "    else:\n",
    "      openpose_conf = -1\n",
    "      if 'OpenPose' in pairs[key]:\n",
    "        openpose_conf = openpose_files[img_name][pairs[key]['OpenPose']][2]\n",
    "        openpose_conf = np.clip(openpose_conf, 0, 1)\n",
    "      mmpose_conf = -1\n",
    "      if 'MMPose' in pairs[key]:\n",
    "        mmpose_conf = mmpose_files[img_name][pairs[key]['MMPose']][2]\n",
    "        mmpose_conf = (mmpose_conf - mmpose_means[key]) / mmpose_stds[key]\n",
    "        mmpose_conf = mmpose_conf * openpose_stds[key] + openpose_means[key]\n",
    "        mmpose_conf = np.clip(mmpose_conf, 0, 1)\n",
    "    \n",
    "      if mmpose_conf > openpose_conf:\n",
    "        blended_current[pairs[key]['OpenPose']][0] = mmpose_files[img_name][pairs[key]['MMPose']][0]\n",
    "        blended_current[pairs[key]['OpenPose']][1] = mmpose_files[img_name][pairs[key]['MMPose']][1]\n",
    "        blended_current[pairs[key]['OpenPose']][2] = mmpose_conf\n",
    "      else:\n",
    "        blended_current[pairs[key]['OpenPose']][0] = openpose_files[img_name][pairs[key]['OpenPose']][0]\n",
    "        blended_current[pairs[key]['OpenPose']][1] = openpose_files[img_name][pairs[key]['OpenPose']][1]\n",
    "        blended_current[pairs[key]['OpenPose']][2] = openpose_conf\n",
    "\n",
    "  blended_current = blended_current.flatten().tolist()\n",
    "  param_dict = {}\n",
    "  param_dict['people'] = [{\"person_id\":[-1]}]\n",
    "  param_dict['people'][0]['pose_keypoints_2d'] = blended_current[:25*3]\n",
    "  param_dict['people'][0]['hand_left_keypoints_2d'] = blended_current[25*3:46*3]\n",
    "  param_dict['people'][0]['hand_right_keypoints_2d'] = blended_current[46*3:67*3]\n",
    "  param_dict['people'][0]['face_keypoints_2d'] = blended_current[67*3:]\n",
    "  with open(os.path.join(BLENDING_RES_DIR, img_name + '_blended.json'), 'w') as outfile:\n",
    "    json.dump(param_dict, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9HabUuqZ7en",
    "outputId": "d2b145d6-8e30-43c4-c206-3a03a0720f52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: cephyr/users/yuanq/Alvis/content/blending_results/MyVideo_1_140_blended.json (deflated 77%)\n",
      "  adding: cephyr/users/yuanq/Alvis/content/blending_results/MyVideo_1_148_blended.json (deflated 76%)\n"
     ]
    }
   ],
   "source": [
    "!zip /cephyr/users/yuanq/Alvis/content/blending_results.zip /cephyr/users/yuanq/Alvis/content/blending_results/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "posPkOQmmCBZ"
   },
   "source": [
    "# Visualization (with thresholding on body keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "31fe965210794957a5367f7094bf8be3",
      "8dae8e17cb9d43baa3015ea4f8158c41",
      "de6249a806994f2cb8d2b99749097eb8"
     ]
    },
    "id": "JCyicMPaT4cG",
    "outputId": "d34932f6-413f-40a1-b129-59f1ebd1f688"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13f8a3cb73448c7a408ed1e49377014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Image:', options=('/mimer/NOBACKUP/groups/snic2022-22-770/Frontiers_framework/Case26/ima…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Select an image in the dropdown menu to visualize its keypoints. It takes a few seconds to render once an image is selected.\n",
    "openpose_threshold = 0.2 # @param {type:\"number\"}\n",
    "mmpose_threshold = 0.5 # @param {type:\"number\"}\\\n",
    "threshold_on = 'body_only' #@param [\"body_only\", \"body+hands\", \"body+hands+face\", \"none\"]\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import glob\n",
    "image_list = glob.glob(os.path.join(IMAGES_PATH, '*'))\n",
    "image_list.sort()\n",
    "w = widgets.Dropdown(\n",
    "    options=image_list,\n",
    "    value=None,\n",
    "    description='Image:',\n",
    ")\n",
    "\n",
    "def on_change(change):\n",
    "  try:\n",
    "    new_index = change['new']['index']\n",
    "    clear_output()\n",
    "    display(w)\n",
    "    curr_fn = image_list[new_index]\n",
    "    img_name = curr_fn.split('/')[-1].split('.')[0]\n",
    "    openpose_detections = read_keypoints(os.path.join(OPENPOSE_RES_DIR, img_name + '_keypoints.json'), True, True, True)\n",
    "    vis_result_openpose = vis_pose_result_body_25(pose_model,\n",
    "                                os.path.join(IMAGES_PATH, img_name + '.jpg'),\n",
    "                                [{'keypoints': openpose_detections}],\n",
    "                                kpt_score_thr=[openpose_threshold if 'body' in threshold_on else 0]*25 + \n",
    "                                [openpose_threshold if 'hands' in threshold_on else 0]*42 + \n",
    "                                [openpose_threshold if 'face' in threshold_on else 0]*68,\n",
    "                                thickness=2,\n",
    "                                show=False)\n",
    "    mmpose_detections = read_keypoints(os.path.join(MMPOSE_RES_DIR, img_name + '_mmpose.json'), True, True, True, orders=['body', 'face', 'hands'])\n",
    "    vis_results_mmpose = vis_pose_result(pose_model,\n",
    "                                os.path.join(IMAGES_PATH, img_name + '.jpg'),\n",
    "                                [{'keypoints': mmpose_detections}],\n",
    "                                dataset=pose_model.cfg.data.test.type,\n",
    "                                kpt_score_thr=[mmpose_threshold if 'body' in threshold_on else 0]*26 + \n",
    "                                [mmpose_threshold if 'hands' in threshold_on else 0]*42 + \n",
    "                                [mmpose_threshold if 'face' in threshold_on else 0]*68,\n",
    "                                thickness=2,\n",
    "                                show=False)\n",
    "    blended_detections = read_keypoints(os.path.join(BLENDING_RES_DIR, img_name + '_blended.json'), True, True, True)\n",
    "    vis_result_blended = vis_pose_result_body_25(pose_model, \n",
    "                                os.path.join(IMAGES_PATH, img_name + '.jpg'),\n",
    "                                [{'keypoints': blended_detections}], \n",
    "                                thickness=2,\n",
    "                                kpt_score_thr=[openpose_threshold if 'body' in threshold_on else 0]*25 + \n",
    "                                [openpose_threshold if 'hands' in threshold_on else 0]*42 + \n",
    "                                [openpose_threshold if 'face' in threshold_on else 0]*68,\n",
    "                                show=False)\n",
    "    print('OpenPose Keypoints')\n",
    "    cv2_imshow(vis_result_openpose)\n",
    "    print('MMPose Keypoints')\n",
    "    cv2_imshow(vis_results_mmpose)\n",
    "    print('Blended Keypoints')\n",
    "    cv2_imshow(vis_result_blended)\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "w.observe(on_change)\n",
    "\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcfcb844fbe47c78553d8238adb27b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(description='Image:', options=('/mimer/NOBACKUP/groups/snic2022-22-770/Frontiers_framework/Case26/image…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d069f99a19e340638109db452c33cd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import glob\n",
    "\n",
    "image_list = glob.glob(os.path.join(IMAGES_PATH, '*'))\n",
    "image_list.sort()\n",
    "\n",
    "w = widgets.Select(\n",
    "    options=image_list,\n",
    "    description='Image:',\n",
    "    rows=10,\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_value_change(change):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        curr_fn = change['new']\n",
    "        img_name = curr_fn.split('/')[-1].split('.')[0]\n",
    "        \n",
    "        openpose_detections = read_keypoints(os.path.join(OPENPOSE_RES_DIR, img_name + '_keypoints.json'), True, True, True)\n",
    "        vis_result_openpose = vis_pose_result_body_25(pose_model,\n",
    "                                    os.path.join(IMAGES_PATH, img_name + '.jpg'),\n",
    "                                    [{'keypoints': openpose_detections}],\n",
    "                                    kpt_score_thr=[openpose_threshold if 'body' in threshold_on else 0]*25 + \n",
    "                                    [openpose_threshold if 'hands' in threshold_on else 0]*42 + \n",
    "                                    [openpose_threshold if 'face' in threshold_on else 0]*68,\n",
    "                                    thickness=2,\n",
    "                                    show=False)\n",
    "        mmpose_detections = read_keypoints(os.path.join(MMPOSE_RES_DIR, img_name + '_mmpose.json'), True, True, True, orders=['body', 'face', 'hands'])\n",
    "        vis_results_mmpose = vis_pose_result(pose_model,\n",
    "                                    os.path.join(IMAGES_PATH, img_name + '.jpg'),\n",
    "                                    [{'keypoints': mmpose_detections}],\n",
    "                                    dataset=pose_model.cfg.data.test.type,\n",
    "                                    kpt_score_thr=[mmpose_threshold if 'body' in threshold_on else 0]*26 + \n",
    "                                    [mmpose_threshold if 'hands' in threshold_on else 0]*42 + \n",
    "                                    [mmpose_threshold if 'face' in threshold_on else 0]*68,\n",
    "                                    thickness=2,\n",
    "                                    show=False)\n",
    "        blended_detections = read_keypoints(os.path.join(BLENDING_RES_DIR, img_name + '_blended.json'), True, True, True)\n",
    "        vis_result_blended = vis_pose_result_body_25(pose_model, \n",
    "                                    os.path.join(IMAGES_PATH, img_name + '.jpg'),\n",
    "                                    [{'keypoints': blended_detections}], \n",
    "                                    thickness=2,\n",
    "                                    kpt_score_thr=[openpose_threshold if 'body' in threshold_on else 0]*25 + \n",
    "                                    [openpose_threshold if 'hands' in threshold_on else 0]*42 + \n",
    "                                    [openpose_threshold if 'face' in threshold_on else 0]*68,\n",
    "                                    show=False)\n",
    "        print('OpenPose Keypoints')\n",
    "        plt.imshow(cv2.cvtColor(vis_result_openpose, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        print('MMPose Keypoints')\n",
    "        plt.imshow(cv2.cvtColor(vis_results_mmpose, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        print('Blended Keypoints')\n",
    "        plt.imshow(cv2.cvtColor(vis_result_blended, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "\n",
    "w.observe(on_value_change, 'value')\n",
    "\n",
    "display(w, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d19aba64d8449788bc872dabc8720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(description='Image:', options=('/mimer/NOBACKUP/groups/snic2022-22-770/Frontiers_framework/Case26/image…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79fb1a24d784dad95f0692b5fddf6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.2, description='OpenPose Threshold:', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13726ad1e9bf4fbabe2913e3dea17970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.5, description='MMPose Threshold:', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29e089d85c84dbda5cac4a97c335508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Threshold On:', options=('body_only', 'body+hands', 'body+hands+face', 'none'), value='b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a64f5030582415ba0ce01220cb62691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import glob\n",
    "\n",
    "image_list = glob.glob(os.path.join(IMAGES_PATH, '*'))\n",
    "image_list.sort()\n",
    "\n",
    "# Widgets\n",
    "w = widgets.Select(\n",
    "    options=image_list,\n",
    "    description='Image:',\n",
    "    rows=10,\n",
    ")\n",
    "\n",
    "openpose_threshold_slider = widgets.FloatSlider(\n",
    "    value=0.2,\n",
    "    min=0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description='OpenPose Threshold:',\n",
    ")\n",
    "\n",
    "mmpose_threshold_slider = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description='MMPose Threshold:',\n",
    ")\n",
    "\n",
    "threshold_on_dropdown = widgets.Dropdown(\n",
    "    options=[\"body_only\", \"body+hands\", \"body+hands+face\", \"none\"],\n",
    "    value=\"body_only\",\n",
    "    description='Threshold On:',\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# Callback function\n",
    "def on_value_change(change):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        curr_fn = change['new']\n",
    "        img_name = curr_fn.split('/')[-1].split('.')[0]\n",
    "        \n",
    "        openpose_detections = read_keypoints(os.path.join(OPENPOSE_RES_DIR, img_name + '_keypoints.json'), True, True, True)\n",
    "        vis_result_openpose = vis_pose_result_body_25(pose_model,\n",
    "                                    os.path.join(IMAGES_PATH, img_name + '.jpg'),\n",
    "                                    [{'keypoints': openpose_detections}],\n",
    "                                    kpt_score_thr=[openpose_threshold_slider.value if 'body' in threshold_on_dropdown.value else 0]*25 + \n",
    "                                    [openpose_threshold_slider.value if 'hands' in threshold_on_dropdown.value else 0]*42 + \n",
    "                                    [openpose_threshold_slider.value if 'face' in threshold_on_dropdown.value else 0]*68,\n",
    "                                    thickness=2,\n",
    "                                    show=False)\n",
    "        mmpose_detections = read_keypoints(os.path.join(MMPOSE_RES_DIR, img_name + '_mmpose.json'), True, True, True, orders=['body', 'face', 'hands'])\n",
    "        vis_results_mmpose = vis_pose_result(pose_model,\n",
    "                                    os.path.join(IMAGES_PATH, img_name + '.jpg'),\n",
    "                                    [{'keypoints': mmpose_detections}],\n",
    "                                    dataset=pose_model.cfg.data.test.type,\n",
    "                                    kpt_score_thr=[mmpose_threshold_slider.value if 'body' in threshold_on_dropdown.value else 0]*26 + \n",
    "                                    [mmpose_threshold_slider.value if 'hands' in threshold_on_dropdown.value else 0]*42 + \n",
    "                                    [mmpose_threshold_slider.value if 'face' in threshold_on_dropdown.value else 0]*68,\n",
    "                                    thickness=2,\n",
    "                                    show=False)\n",
    "        blended_detections = read_keypoints(os.path.join(BLENDING_RES_DIR, img_name + '_blended.json'), True, True, True)\n",
    "        vis_result_blended = vis_pose_result_body_25(pose_model, \n",
    "                                    os.path.join(IMAGES_PATH, img_name + '.jpg'),\n",
    "                                    [{'keypoints': blended_detections}], \n",
    "                                    thickness=2,\n",
    "                                    kpt_score_thr=[openpose_threshold_slider.value if 'body' in threshold_on_dropdown.value else 0]*25 + \n",
    "                                    [openpose_threshold_slider.value if 'hands' in threshold_on_dropdown.value else 0]*42 + \n",
    "                                    [openpose_threshold_slider.value if 'face' in threshold_on_dropdown.value else 0]*68,\n",
    "                                    show=False)\n",
    "        print('OpenPose Keypoints')\n",
    "        plt.imshow(cv2.cvtColor(vis_result_openpose, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        print('MMPose Keypoints')\n",
    "        plt.imshow(cv2.cvtColor(vis_results_mmpose, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        print('Blended Keypoints')\n",
    "        plt.imshow(cv2.cvtColor(vis_result_blended, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "\n",
    "w.observe(on_value_change, 'value')\n",
    "\n",
    "# Display widgets\n",
    "display(w, openpose_threshold_slider, mmpose_threshold_slider, threshold_on_dropdown, output)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPDhc/vIlJocSf/zPpaiifh",
   "collapsed_sections": [
    "4sHhW5MVUVV_",
    "uILcwa7kpnW2",
    "3wcbef2rmUDz",
    "PopS_NenDdwT",
    "1N5RCzhkmSGf"
   ],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "009a800baa884ef9b857d651db49e512": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31fe965210794957a5367f7094bf8be3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "/content/images/images/01_cropped.jpg",
       "/content/images/images/02_cropped.jpg",
       "/content/images/images/03_cropped.jpg",
       "/content/images/images/04_cropped.jpg",
       "/content/images/images/05_cropped.jpg",
       "/content/images/images/06_cropped.jpg",
       "/content/images/images/07_cropped.jpg",
       "/content/images/images/08_cropped.jpg",
       "/content/images/images/09_cropped.jpg",
       "/content/images/images/100_cropped.jpg",
       "/content/images/images/10_cropped.jpg",
       "/content/images/images/11_cropped.jpg",
       "/content/images/images/12_cropped.jpg",
       "/content/images/images/13_cropped.jpg",
       "/content/images/images/14_cropped.jpg",
       "/content/images/images/15_cropped.jpg",
       "/content/images/images/16_cropped.jpg",
       "/content/images/images/17_cropped.jpg",
       "/content/images/images/18_cropped.jpg",
       "/content/images/images/19_cropped.jpg",
       "/content/images/images/20_cropped.jpg",
       "/content/images/images/21_cropped.jpg",
       "/content/images/images/22_cropped.jpg",
       "/content/images/images/23_cropped.jpg",
       "/content/images/images/24_cropped.jpg",
       "/content/images/images/25_cropped.jpg",
       "/content/images/images/26_cropped.jpg",
       "/content/images/images/27_cropped.jpg",
       "/content/images/images/28_cropped.jpg",
       "/content/images/images/29_cropped.jpg",
       "/content/images/images/30_cropped.jpg",
       "/content/images/images/31_cropped.jpg",
       "/content/images/images/32_cropped.jpg",
       "/content/images/images/33_cropped.jpg",
       "/content/images/images/34_cropped.jpg",
       "/content/images/images/35_cropped.jpg",
       "/content/images/images/36_cropped.jpg",
       "/content/images/images/37_cropped.jpg",
       "/content/images/images/38_cropped.jpg",
       "/content/images/images/39_cropped.jpg",
       "/content/images/images/40_cropped.jpg",
       "/content/images/images/41_cropped.jpg",
       "/content/images/images/42_cropped.jpg",
       "/content/images/images/43_cropped.jpg",
       "/content/images/images/44_cropped.jpg",
       "/content/images/images/45_cropped.jpg",
       "/content/images/images/46_cropped.jpg",
       "/content/images/images/47_cropped.jpg",
       "/content/images/images/48_cropped.jpg",
       "/content/images/images/49_cropped.jpg",
       "/content/images/images/50_cropped.jpg",
       "/content/images/images/51_cropped.jpg",
       "/content/images/images/52_cropped.jpg",
       "/content/images/images/53_cropped.jpg",
       "/content/images/images/54_cropped.jpg",
       "/content/images/images/55_cropped.jpg",
       "/content/images/images/56_cropped.jpg",
       "/content/images/images/57_cropped.jpg",
       "/content/images/images/58_cropped.jpg",
       "/content/images/images/59_cropped.jpg",
       "/content/images/images/60_cropped.jpg",
       "/content/images/images/61_cropped.jpg",
       "/content/images/images/62_cropped.jpg",
       "/content/images/images/63_cropped.jpg",
       "/content/images/images/64_cropped.jpg",
       "/content/images/images/65_cropped.jpg",
       "/content/images/images/66_cropped.jpg",
       "/content/images/images/67_cropped.jpg",
       "/content/images/images/68_cropped.jpg",
       "/content/images/images/69_cropped.jpg",
       "/content/images/images/70_cropped.jpg",
       "/content/images/images/71_cropped.jpg",
       "/content/images/images/72_cropped.jpg",
       "/content/images/images/73_cropped.jpg",
       "/content/images/images/74_cropped.jpg",
       "/content/images/images/75_cropped.jpg",
       "/content/images/images/76_cropped.jpg",
       "/content/images/images/77_cropped.jpg",
       "/content/images/images/78_cropped.jpg",
       "/content/images/images/79_cropped.jpg",
       "/content/images/images/80_cropped.jpg",
       "/content/images/images/81_cropped.jpg",
       "/content/images/images/82_cropped.jpg",
       "/content/images/images/83_cropped.jpg",
       "/content/images/images/84_cropped.jpg",
       "/content/images/images/85_cropped.jpg",
       "/content/images/images/86_cropped.jpg",
       "/content/images/images/87_cropped.jpg",
       "/content/images/images/88_cropped.jpg",
       "/content/images/images/89_cropped.jpg",
       "/content/images/images/90_cropped.jpg",
       "/content/images/images/91_cropped.jpg",
       "/content/images/images/92_cropped.jpg",
       "/content/images/images/93_cropped.jpg",
       "/content/images/images/94_cropped.jpg",
       "/content/images/images/95_cropped.jpg",
       "/content/images/images/96_cropped.jpg",
       "/content/images/images/97_cropped.jpg",
       "/content/images/images/98_cropped.jpg",
       "/content/images/images/99_cropped.jpg"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Image:",
      "description_tooltip": null,
      "disabled": false,
      "index": 34,
      "layout": "IPY_MODEL_8dae8e17cb9d43baa3015ea4f8158c41",
      "style": "IPY_MODEL_de6249a806994f2cb8d2b99749097eb8"
     }
    },
    "390a2ba23c9f4abfb7566d87368ca96f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49ff66d45ce9430bab637e510a915f98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d32e9eac96944a08c3ec0ec1124c015",
      "placeholder": "​",
      "style": "IPY_MODEL_a7b5662f6ec547038a2ca3a4c7b67dab",
      "value": "100%"
     }
    },
    "4e124465e1ca42059cda022fa78618e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "68e61eeefd034219a483e9f0347f26c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75a11e28a5fb49cb818d76d4cc3b4767",
      "placeholder": "​",
      "style": "IPY_MODEL_7fe716927ba044c0afa588f87a45e95a",
      "value": "100%"
     }
    },
    "6d32e9eac96944a08c3ec0ec1124c015": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70f026f132e143eca82ecdbab127a126": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75a11e28a5fb49cb818d76d4cc3b4767": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "798be96747fa41b784565aef4d247296": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7d954a770a664caeae10d48a9324c49e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_009a800baa884ef9b857d651db49e512",
      "max": 167287506,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4e124465e1ca42059cda022fa78618e9",
      "value": 167287506
     }
    },
    "7fe716927ba044c0afa588f87a45e95a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "819211b258314224918b09ee27f625ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_68e61eeefd034219a483e9f0347f26c2",
       "IPY_MODEL_7d954a770a664caeae10d48a9324c49e",
       "IPY_MODEL_f6ad6df56b0f4afbb0d5e7f0ea686a60"
      ],
      "layout": "IPY_MODEL_bd52b8cd96fa4dda817aaa45e08bed5e"
     }
    },
    "8dae8e17cb9d43baa3015ea4f8158c41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "911d276ce0e1456d907aa91e83b366c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a4fca8053be4e1d8b729ff6118a9c45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_49ff66d45ce9430bab637e510a915f98",
       "IPY_MODEL_d35864af678f4aab8367c98275669298",
       "IPY_MODEL_bc7c3bbb8d594e9bb7a0bc3f672288ba"
      ],
      "layout": "IPY_MODEL_f56f80425ffe4e7dbc6e155ee85142d6"
     }
    },
    "a69c23a58aa6471996f1845ec47b3dca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7b5662f6ec547038a2ca3a4c7b67dab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc7c3bbb8d594e9bb7a0bc3f672288ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de9527c232d047219c8fae4383afb5a5",
      "placeholder": "​",
      "style": "IPY_MODEL_70f026f132e143eca82ecdbab127a126",
      "value": " 243M/243M [00:22&lt;00:00, 11.9MB/s]"
     }
    },
    "bd52b8cd96fa4dda817aaa45e08bed5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d35864af678f4aab8367c98275669298": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_390a2ba23c9f4abfb7566d87368ca96f",
      "max": 255239125,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_798be96747fa41b784565aef4d247296",
      "value": 255239125
     }
    },
    "de6249a806994f2cb8d2b99749097eb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de9527c232d047219c8fae4383afb5a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f56f80425ffe4e7dbc6e155ee85142d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6ad6df56b0f4afbb0d5e7f0ea686a60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_911d276ce0e1456d907aa91e83b366c8",
      "placeholder": "​",
      "style": "IPY_MODEL_a69c23a58aa6471996f1845ec47b3dca",
      "value": " 160M/160M [00:16&lt;00:00, 8.80MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
